{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# defining and showing my environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False,render_mode='human')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n",
      "16\n",
      "Discrete(4)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# no of our possible states\n",
    "print(env.observation_space)\n",
    "# print the states in the environment\n",
    "print(env.observation_space.n)\n",
    "\n",
    "# no of possible actions\n",
    "print(env.action_space)\n",
    "# what are the actions in the environment\n",
    "print(env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 0, 0.0, False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analysing the transition probability and reward function\n",
    "# it means that in state 0 what is the probability of taking a action 2\n",
    "env.P[0][3]\n",
    "# it returns (probability, nextstate, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0.0, False, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets take the first step\n",
    "# env.reset()\n",
    "env.step(1)\n",
    "# it returns (nextstate, reward, done, _,info)\n",
    "# info contains the probability of the action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4x4=16 States \n",
    "\n",
    "Left, Right, Up, Down 4 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.randint(0,2)\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random action selection and also generating episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our random_action is  1\n",
      "the current state is  4\n",
      "our random_action is  2\n",
      "we are done\n",
      "the current state is  5\n",
      "our random_action is  1\n",
      "the current state is  4\n",
      "our random_action is  1\n",
      "the current state is  8\n",
      "our random_action is  1\n",
      "we are done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m done\u001b[39m==\u001b[39m\u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwe are done\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m     env\u001b[39m.\u001b[39mreset()  \n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe current state is  \u001b[39m\u001b[39m{\u001b[39;00mnext_state\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "now=time.time()\n",
    "while(1):\n",
    "    if time.time()-now>10:\n",
    "        break\n",
    "    # random_action = env.action_space.sample()\n",
    "    # print(\"our random_action is \", random_action)\n",
    "    random_action=np.random.randint(1,3)\n",
    "    print(\"our random_action is \", random_action)\n",
    "    next_state, reward, done,_ ,info = env.step(random_action)\n",
    "    if next_state in [5,7,11,12]:\n",
    "        env.reset()\n",
    "    if next_state==15:\n",
    "        print(\"hurray we reached the goal\")\n",
    "        time.sleep(5)\n",
    "        env.reset()  \n",
    "    if done==True:\n",
    "        print(\"we are done\")\n",
    "        time.sleep(5)\n",
    "        env.reset()  \n",
    "    print(f\"the current state is  {next_state}\")\n",
    "\n",
    "# if next_state==15:\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# value iteration method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 13, 0.0, False)],\n",
       " 1: [(1.0, 14, 0.0, False)],\n",
       " 2: [(1.0, 15, 1.0, True)],\n",
       " 3: [(1.0, 10, 0.0, False)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged at iteration  7\n",
      "final value function table is  [0.59049 0.6561  0.729   0.6561  0.6561  0.      0.81    0.      0.729\n",
      " 0.81    0.9     0.      0.      0.9     1.      0.     ]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(env):\n",
    "    num_iterations = 1000\n",
    "    # break point \n",
    "    threshold=1e-20\n",
    "    # discount factor\n",
    "    gamma = 0.9\n",
    "    \n",
    "    # initialize the value function- it will equal to no of state possible\n",
    "    value_function_table=np.zeros(env.observation_space.n)    \n",
    "    for i in range(num_iterations):\n",
    "        updated_value_function_table=np.copy(value_function_table)\n",
    "        # computing the q_value for all possible action for each state\n",
    "        for state in range(env.observation_space.n):\n",
    "            # printing how the value function is changing\n",
    "\n",
    "            q_value=[sum([prob*(reward+gamma*updated_value_function_table[next_state]) for prob,next_state,reward,_ in env.P[state][action]]) for action in range(env.action_space.n)]\n",
    "            # update the value function\n",
    "            value_function_table[state]=max(q_value)\n",
    "        # check the convergence\n",
    "        if (np.sum(np.fabs(updated_value_function_table-value_function_table))<threshold):\n",
    "            print(\"converged at iteration \",i+1)\n",
    "            print(\"final value function table is \",updated_value_function_table)\n",
    "            break\n",
    "    return value_function_table\n",
    "\n",
    "# getting the optimal value function\n",
    "optimal_value_function=value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59049, 0.6561 , 0.729  , 0.6561 , 0.6561 , 0.     , 0.81   ,\n",
       "       0.     , 0.729  , 0.81   , 0.9    , 0.     , 0.     , 0.9    ,\n",
       "       1.     , 0.     ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets see the optimal policy\n",
    "# env = gym.make(\"FrozenLake-v1\", is_slippery=False,render_mode='human')\n",
    "# env.reset()\n",
    "# env.render()\n",
    "# curr_state=1\n",
    "# while(1):\n",
    "#     # choosing the optimal_value -we will move where the value_function is max\n",
    "#     if max(optimal_value_function[curr_state+1],optimal_value_function[curr_state+2],optimal_value_function[curr_state+3],optimal_value_function[curr_state+4])==optimal_value_function[curr_state+1] and curr_state<=15:\n",
    "#         # move right\n",
    "#         action=2\n",
    "#     elif max(optimal_value_function[curr_state+1],optimal_value_function[curr_state+2],optimal_value_function[curr_state+3],optimal_value_function[curr_state+4])==optimal_value_function[curr_state+2]:\n",
    "#         # move left\n",
    "#         action=3\n",
    "#     elif max(optimal_value_function[curr_state+1],optimal_value_function[curr_state+2],optimal_value_function[curr_state+3],optimal_value_function[curr_state+4])==optimal_value_function[curr_state+3]:\n",
    "#         # move up\n",
    "#         action=0\n",
    "#     else:\n",
    "#         # move down\n",
    "#         action=1\n",
    "#     next_state,reward,done,_,_=env.step(action)\n",
    "#     curr_state=next_state\n",
    "#     if curr_state==16:\n",
    "#         print(\"hurray we reached the goal\")\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using value function to generate the policy now\n",
    "def extract_policy(value_function_table, gamma=1.0):\n",
    "    # initialize the policy with zeros\n",
    "    policy=np.zeros(env.observation_space.n)\n",
    "    for state in range(env.observation_space.n):\n",
    "        # initialize the q_table for each state\n",
    "        q_table=np.zeros(env.action_space.n)\n",
    "        # compute the q_value for all possible actions in that state\n",
    "        for action in range(env.action_space.n):\n",
    "            for next_sr in env.P[state][action]:\n",
    "                # next_sr is a tuple of (probability, nextstate, reward, done)\n",
    "                prob,next_state,reward,done=next_sr\n",
    "                q_table[action]+=(prob*(reward+gamma*value_function_table[next_state]))\n",
    "        # select the action which has the maximum q_value\n",
    "        policy[state]=np.argmax(q_table)\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_policy=extract_policy(optimal_value_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 1., 0., 1., 0., 1., 0., 2., 1., 1., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "optimum_policy[14]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_state=env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 1., 0., 1., 0., 1., 0., 2., 1., 1., 0., 0., 2., 2., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimum_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n",
      "8\n",
      "9\n",
      "13\n",
      "14\n",
      "15\n",
      "hurray we reached the goal\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False,render_mode='human')\n",
    "env.reset()\n",
    "env.render()\n",
    "while(1):\n",
    "    next_state,reward,done,_,_=env.step(int(optimum_policy[curr_state]))\n",
    "    # next_state,reward,done,_,_=env.step(2)\n",
    "    curr_state=next_state\n",
    "    print(curr_state)\n",
    "    if curr_state==15:\n",
    "        print(\"hurray we reached the goal\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### policy_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env):\n",
    "    num_iterations = 1000\n",
    "    threshold = 1e-20\n",
    "    gamma = 0.9\n",
    "    \n",
    "    policy_table = np.zeros(env.observation_space.n)\n",
    "    value_function_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # policy evaluation\n",
    "        for state in range(env.observation_space.n):\n",
    "            q_value = [sum([prob * (reward + gamma * value_function_table[next_state]) for prob, next_state, reward, _ in env.P[state][policy_table[state]]])]\n",
    "            value_function_table[state] = q_value\n",
    "        \n",
    "        # policy improvement\n",
    "        policy_stable = True\n",
    "        for state in range(env.observation_space.n):\n",
    "            old_action = policy_table[state]\n",
    "            q_value = [sum([prob * (reward + gamma * value_function_table[next_state]) for prob, next_state, reward, _ in env.P[state][action]]) for action in range(env.action_space.n)]\n",
    "            policy_table[state] = np.argmax(q_value)\n",
    "            if old_action != policy_table[state]:\n",
    "                policy_stable = False\n",
    "        \n",
    "        if policy_stable:\n",
    "            print(\"converged at iteration \", i + 1)\n",
    "            print(\"final value function table is \", value_function_table)\n",
    "            break\n",
    "            \n",
    "    return policy_table\n",
    "\n",
    "# getting the optimal policy function\n",
    "optimal_policy_function = policy_iteration(env)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "11e50c94871236293453a37a38b38aac5bdf2bcb2b7abe317aadc69c854d36a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
